import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import time
import warnings
warnings.filterwarnings('ignore')

# Upload the file
uploaded = files.upload()
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)

# Create the output directory if it doesn't exist
output_dir = './outputs/GradientBoostingRegressor_Grouped'
os.makedirs(output_dir, exist_ok=True)

print("Dataset loaded successfully!")
print(f"Dataset shape: {data.shape}")

# feature selection
print("\n" + "="*60)
print("FEATURE SELECTION")
print("="*60)

# Define the input features
input_feature_names = [
    'game_id', 
    'completion_rate', 
    'revisit_frequency', 
    'time_spent_same_domain', 
    'content_same_domain'
]

print(f"Looking for input features: {input_feature_names}")

# Select input features
X = data[available_features]
print(f"Final input features shape: {X.shape}")

# Target output grouping
# Group the games into 3 groups (10 games each)
group1_cols = game_columns[:10]   # Games 1-10
group2_cols = game_columns[10:20] # Games 11-20  
group3_cols = game_columns[20:30] # Games 21-30

groups = {
    'Group 1 (Games 1-10)': group1_cols,
    'Group 2 (Games 11-20)': group2_cols,
    'Group 3 (Games 21-30)': group3_cols
}

print(f"Group 1: {len(group1_cols)} games")
print(f"Group 2: {len(group2_cols)} games") 
print(f"Group 3: {len(group3_cols)} games")

# DATA PREPROCESSING
# Normalise the input features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset
X_train, X_test, y_train_all, y_test_all = train_test_split(
    X_scaled, y_all, test_size=0.2, random_state=42
)

print(f"Training set size: {X_train.shape[0]}")
print(f"Test set size: {X_test.shape[0]}")

# TRAIN MODELS FOR EACH GROUP
group_results = {}
group_models = {}
all_predictions = {}

for group_name, group_cols in groups.items():
    print("\n" + "="*80)
    print(f"TRAINING: {group_name}")
    print("="*80)
    
    if len(group_cols) == 0:
        print(f"No columns for {group_name}, skipping...")
        continue
    
    # Get target data for this group
    y_train_group = y_train_all[group_cols]
    y_test_group = y_test_all[group_cols]
    
    print(f"Target shape for {group_name}: {y_train_group.shape}")
    
    
    # GRADIENT BOOSTING MODEL
    
    print(f"\nTraining Gradient Boosting Model for {group_name}...")
    
    # Gradient Boosting parameters
    gb_model = GradientBoostingRegressor(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=4,
        min_samples_split=5,
        min_samples_leaf=2,
        subsample=0.8,
        random_state=42
    )
    
    # Wrap with MultiOutputRegressor for multiple targets
    multi_model = MultiOutputRegressor(gb_model)
    
    start_time = time.time()
    multi_model.fit(X_train, y_train_group)
    training_time = time.time() - start_time
    
    # Make predictions
    y_pred = multi_model.predict(X_test)
    
    # Calculate metrics
    rmse = np.sqrt(mean_squared_error(y_test_group, y_pred))
    r2 = r2_score(y_test_group, y_pred)
    
    print(f'RMSE: {rmse:.4f}')
    print(f'R²: {r2:.4f}')
    print(f'Training time: {training_time:.2f} seconds')
    
    # Store results
    group_results[group_name] = {
        'rmse': rmse,
        'r2': r2,
        'training_time': training_time,
        'n_targets': len(group_cols)
    }
    
    group_models[group_name] = multi_model
    all_predictions[group_name] = {
        'predictions': y_pred,
        'actual': y_test_group.values
    }

# overall results summary
results_summary = pd.DataFrame({
    'Group': list(group_results.keys()),
    'RMSE': [group_results[g]['rmse'] for g in group_results.keys()],
    'R²': [group_results[g]['r2'] for g in group_results.keys()],
    'Training_Time_sec': [group_results[g]['training_time'] for g in group_results.keys()],
    'Num_Targets': [group_results[g]['n_targets'] for g in group_results.keys()]
})

print(results_summary.to_string(index=False, float_format='%.4f'))

# Calculate overall averages
avg_rmse = np.mean([group_results[g]['rmse'] for g in group_results.keys()])
avg_r2 = np.mean([group_results[g]['r2'] for g in group_results.keys()])
total_time = sum([group_results[g]['training_time'] for g in group_results.keys()])

print(f"\nOVERALL PERFORMANCE:")
print(f"Average RMSE across groups: {avg_rmse:.4f}")
print(f"Average R² across groups: {avg_r2:.4f}")
print(f"Total training time: {total_time:.2f} seconds")

# Find best and worst performing groups
best_group_rmse = min(group_results.keys(), key=lambda x: group_results[x]['rmse'])
best_group_r2 = max(group_results.keys(), key=lambda x: group_results[x]['r2'])
worst_group_rmse = max(group_results.keys(), key=lambda x: group_results[x]['rmse'])

print(f"\nBest RMSE: {best_group_rmse} ({group_results[best_group_rmse]['rmse']:.4f})")
print(f"Best R²: {best_group_r2} ({group_results[best_group_r2]['r2']:.4f})")
print(f"Worst RMSE: {worst_group_rmse} ({group_results[worst_group_rmse]['rmse']:.4f})")

# Visualisations
# 1. Group Performance Comparison
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))

groups_names = list(group_results.keys())
rmse_values = [group_results[g]['rmse'] for g in groups_names]
r2_values = [group_results[g]['r2'] for g in groups_names]
time_values = [group_results[g]['training_time'] for g in groups_names]

# RMSE by Group
bars1 = ax1.bar(range(len(groups_names)), rmse_values, 
                color=['red', 'orange', 'green'], alpha=0.7, edgecolor='black')
ax1.set_title('RMSE by Group', fontsize=14, fontweight='bold')
ax1.set_ylabel('RMSE', fontsize=12)
ax1.set_xticks(range(len(groups_names)))
ax1.set_xticklabels([g.replace(' (Games ', '\n(Games ') for g in groups_names])
ax1.grid(True, axis='y', alpha=0.3)
for i, v in enumerate(rmse_values):
    ax1.text(i, v + max(rmse_values)*0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')

# R² by Group
bars2 = ax2.bar(range(len(groups_names)), r2_values, 
                color=['red', 'orange', 'green'], alpha=0.7, edgecolor='black')
ax2.set_title('R² Score by Group', fontsize=14, fontweight='bold')
ax2.set_ylabel('R² Score', fontsize=12)
ax2.set_xticks(range(len(groups_names)))
ax2.set_xticklabels([g.replace(' (Games ', '\n(Games ') for g in groups_names])
ax2.grid(True, axis='y', alpha=0.3)
for i, v in enumerate(r2_values):
    ax2.text(i, v + max(r2_values)*0.01, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')

# 2. Predicted vs Actual for each group (first game in each group)
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, (group_name, predictions) in enumerate(all_predictions.items()):
    ax = axes[idx]
    
    # Plot for first game in the group
    actual_vals = predictions['actual'][:, 0]
    pred_vals = predictions['predictions'][:, 0]
    
    ax.scatter(actual_vals, pred_vals, c=pred_vals, cmap='coolwarm', 
              s=50, edgecolors='k', alpha=0.7)
    
    # Diagonal line
    min_val = min(actual_vals.min(), pred_vals.min())
    max_val = max(actual_vals.max(), pred_vals.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Perfect Prediction')
    
    # Calculate metrics for this specific game
    game_rmse = np.sqrt(mean_squared_error(actual_vals, pred_vals))
    game_r2 = r2_score(actual_vals, pred_vals)
    
    ax.set_title(f'{group_name}\nFirst Game: RMSE={game_rmse:.3f}, R²={game_r2:.3f}', 
                fontweight='bold')
    ax.set_xlabel('Actual Values')
    ax.set_ylabel('Predicted Values')
    ax.grid(True, alpha=0.3)
    ax.legend()

plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'predictions_by_group.png'), dpi=300, bbox_inches='tight')
plt.show()

# 3. Feature Importance
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

for idx, (group_name, model) in enumerate(group_models.items()):
    ax = axes[idx]
    
    # Get feature importances averaged across all estimators in the group
    importances = np.array([est.feature_importances_ for est in model.estimators_])
    avg_importance = importances.mean(axis=0)
    
    # Sort features by importance
    sorted_idx = np.argsort(avg_importance)[::-1]
    sorted_features = [available_features[i] for i in sorted_idx]
    sorted_importance = avg_importance[sorted_idx]
    
    # Create bar plot
    bars = ax.bar(range(len(available_features)), sorted_importance, 
                  color='steelblue', alpha=0.8, edgecolor='black')
    
    ax.set_title(f'Feature Importance\n{group_name}', fontweight='bold')
    ax.set_xlabel('Features')
    ax.set_ylabel('Importance')
    ax.set_xticks(range(len(available_features)))
    ax.set_xticklabels(sorted_features, rotation=45, ha='right')
    ax.grid(True, axis='y', alpha=0.3)
    
    # Add value labels on bars
    for i, v in enumerate(sorted_importance):
        ax.text(i, v + max(sorted_importance)*0.01, f'{v:.3f}', 
               ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.savefig(os.path.join(output_dir, 'feature_importance_by_group.png'), dpi=300, bbox_inches='tight')
plt.show()

print(f"Average performance across all groups:")
print(f"- RMSE: {avg_rmse:.4f}")
print(f"- R²: {avg_r2:.4f}")
